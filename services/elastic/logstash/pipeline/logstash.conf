input {

  file {
    path => "/app/logstash/monitor/*.log"
    start_position => "beginning"
  }

}
filter {

  # create hash from message to handle duplicates
  fingerprint {
    source => "message"
    target => "[@metadata][fingerprint]"
    method => "MURMUR3"
  }

  # add environment
  mutate {
      add_field => { "[log][environment]" => "${ENVIRONMENT_SUFFIX}" }
  }

  mutate {
    lowercase => [ "[log][environment]" ]
  }

  if [path] =~ "access" {
    

    mutate {
      add_field => { "[log][type]" => "access" }
    }
    
    grok {
      match => { "message" => ["%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[@metadata][jwt]}\" \"%{DATA:[nginx][access][agent]}\" \"%{GREEDYDATA:[nginx][access][client_ip]}\" \"%{GREEDYDATA:[nginx][access][payload]}\""] }
      remove_field => "message"
    }
    mutate {
      add_field => { "read_timestamp" => "%{@timestamp}" }
    }
    date {
      match => [ "[nginx][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
      remove_field => "[nginx][access][time]"
    }
    useragent {
      source => "[nginx][access][agent]"
      target => "[nginx][access][user_agent]"
      remove_field => "[nginx][access][agent]"
    }
    geoip {
      source => "[nginx][access][remote_ip]"
      target => "[nginx][access][geoip]"
    }

    # decode JWT
    if [@metadata][jwt] {

      mutate {
        split => { "[@metadata][jwt]" => "." }
      }

      mutate {
        copy => { "[@metadata][jwt][1]" => "[@metadata][b64]" }
      }

      ruby {
        init => "require 'base64'"
        code => "event.set('[@metadata][jwt_decoded]', Base64.decode64(event.get('[@metadata][b64]')))"
      }

      json {
        source => "[@metadata][jwt_decoded]"
        target => "jwt_data"
      }
    }

    }

    if [nginx][access][payload] {
      # remove \ from payload
      mutate {
        gsub => [
          "[nginx][access][payload]", "[\\]", ""
        ]
      }

      # parse json to data field
      json {
        source => "[nginx][access][payload]"
        target => "[data]"
      }
    }


    # Get the type of operation of interest

    mutate {
      add_field => { "[@metadata][action]" => "%{[nginx][access][method]} %{[nginx][access][url]}" }
    }

    # Get the operation
    translate {
      field => "[@metadata][action]"
      destination => "operation"
      dictionary_path => "/app/logstash/dictionaries/action.yml"
      fallback => "other"
      regex => true
    }

    # Extract mine guid based on operation
    if [operation] != "other" {
      grok {
        match => { "[nginx][access][url]" => [".*/api/mines/%{UUID:[mine][id]}", ".*/api/documents/mines/%{UUID:[mine][id]}"] }
      }

      if [mine][id] {
            # lookup by mine id
        jdbc_streaming {
          jdbc_driver_library => "/app/logstash/jdbc/postgresql-42.2.5.jar"
          jdbc_driver_class => "org.postgresql.Driver"
          jdbc_connection_string => "${JDBC_CONNECTION_STRING}"
          jdbc_user => "${JDBC_USER}"
          jdbc_password => "${JDBC_PASSWORD}"
          cache_expiration => 300.0
          statement => "SELECT mi.mine_region as region, mi.latitude as lat, mi.longitude as lon, mi.mine_no as number, mi.mine_name as name FROM mine mi WHERE mi.mine_guid = :mine_id  AND mi.deleted_ind = false;"
          parameters => { "mine_id" => "[mine][id]"}
          target => "mine_lookup_result"
        }


        if [mine_lookup_result] and [mine_lookup_result][0] {
          
          mutate {
            convert => {
              "[mine_lookup_result][0][lat]" => "float"
              "[mine_lookup_result][0][lon]" => "float"
            }
          }

          mutate {
            add_field => {
              "[mine_data][region]" => "%{[mine_lookup_result][0][region]}"
              "[mine_data][number]" => "%{[mine_lookup_result][0][number]}"
              "[mine_data][name]" => "%{[mine_lookup_result][0][name]}"
              "[mine_data][location][lat]" => "%{[mine_lookup_result][0][lat]}"
              "[mine_data][location][lon]" => "%{[mine_lookup_result][0][lon]}"
            }
          }


          mutate {
            remove_field => [ "mine_lookup_result" ]
          }
        }
      }
      if [nginx][access][referrer] {
        translate {
          field => "[nginx][access][referrer]"
          destination => "[mds][group]"
          dictionary_path => "/app/logstash/dictionaries/group.yml"
          regex => true
          fallback => 'other'
          exact => true
        }
      }

    grok {
      match => { "message" => ["%{DATA:[nginx][error][time]} \[%{DATA:[nginx][error][level]}\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}"] }
      remove_field => "message"
    }
  }
  else if [path] =~ "error" {

    mutate {
      add_field => { "[log][type]" => "error" }
    }

    grok {
      match => { "message" => ["%{DATA:[nginx][error][time]} \[%{DATA:[nginx][error][level]}\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}"] }
      remove_field => "message"
    }
    mutate {
      rename => { "@timestamp" => "read_timestamp" }
    }
    date {
      match => [ "[nginx][error][time]", "YYYY/MM/dd H:m:s" ]
      remove_field => "[nginx][error][time]"
    }
  }
}

output {
  
  stdout { codec => rubydebug}

  elasticsearch {
    hosts => "${ELASTIC_HOST}"
    user => "${ELASTIC_USERNAME}"
    password => "${ELASTIC_PASSWORD}"
    index => "mds-nginx-%{[log][type]}-%{[log][environment]}-%{+YYYY.MM}"
    document_id => "%{[@metadata][fingerprint]}"
    template => "/app/logstash/templates/mds.nginx.template.json"
    template_name => "mds-nginx"
  }

}
